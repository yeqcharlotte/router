DEBUG 10-17 09:29:08 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:08 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:08 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:08 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:08 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:08 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:08 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:15 [entrypoints/utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 10-17 09:29:15 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:15 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:15 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
WARNING 10-17 09:29:15 [utils/__init__.py:1742] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:15 [entrypoints/openai/api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:15 [entrypoints/utils.py:233] non-default args: {'model_tag': 'deepseek-ai/DeepSeek-V3-0324', 'host': '0.0.0.0', 'port': 20005, 'disable_uvicorn_access_log': True, 'model': 'deepseek-ai/DeepSeek-V3-0324', 'trust_remote_code': True, 'data_parallel_size': 8, 'enable_expert_parallel': True, 'async_scheduling': True, 'kv_transfer_config': KVTransferConfig(kv_connector='NixlConnector', engine_id='da373fea-13d3-4528-92f5-f0d0b154edcc', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'backends': ['UCX', 'GDS']}, kv_connector_module_path=None), 'compilation_config': {"level":null,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":null,"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":null,"local_cache_dir":null}}
[1;36m(APIServer pid=1)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:15 [transformers_utils/config.py:388] Replacing legacy 'type' key with 'rope_type'
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:16 [model_executor/models/registry.py:450] Cached model info file for class vllm.model_executor.models.deepseek_v2.DeepseekV3ForCausalLM not found
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:16 [model_executor/models/registry.py:503] Cache model info for class vllm.model_executor.models.deepseek_v2.DeepseekV3ForCausalLM miss. Loading model instead.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:21 [model_executor/models/registry.py:511] Loaded model info for class vllm.model_executor.models.deepseek_v2.DeepseekV3ForCausalLM
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:21 [logging_utils/log_time.py:27] Registry inspect model class: Elapsed time 5.4924767 secs
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:21 [config/model.py:547] Resolved architecture: DeepseekV3ForCausalLM
[1;36m(APIServer pid=1)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:21 [config/model.py:1510] Using max model len 163840
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:21 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:22 [engine/arg_utils.py:1672] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:22 [engine/arg_utils.py:1681] Setting max_num_seqs to 1024 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:22 [engine/arg_utils.py:1293] Defaulting to mp-based distributed executor backend for async scheduling.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:22 [config/scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:22 [platforms/cuda.py:166] Forcing kv cache block size to 64 for FlashMLA backend.
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:23 [transformers_utils/config.py:388] Replacing legacy 'type' key with 'rope_type'
[1;36m(APIServer pid=1)[0;0m INFO 10-17 09:29:23 [v1/engine/utils.py:651] Started DP Coordinator process (PID: 269)
DEBUG 10-17 09:29:26 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:26 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:26 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:26 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:26 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:26 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:26 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:26 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:26 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:26 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:26 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:26 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:26 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:26 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:26 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:26 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:26 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:27 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:27 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:27 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:27 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:27 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:27 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:27 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:27 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:27 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:27 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:27 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:27 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:27 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:27 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:27 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:27 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:27 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:27 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:27 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:27 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:27 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:27 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:27 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:28 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:28 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:28 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:28 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:28 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:28 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:28 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:28 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:28 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:28 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:28 [platforms/__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP5 pid=277)[0;0m INFO 10-17 09:29:30 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:30 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:30 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:30 [v1/engine/utils.py:859] HELLO from local core engine process 5.
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:30 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp5
[1;36m(EngineCore_DP6 pid=278)[0;0m INFO 10-17 09:29:31 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:31 [v1/engine/utils.py:859] HELLO from local core engine process 6.
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp6
[1;36m(EngineCore_DP4 pid=276)[0;0m INFO 10-17 09:29:31 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:31 [v1/engine/utils.py:859] HELLO from local core engine process 4.
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp4
[1;36m(EngineCore_DP1 pid=273)[0;0m INFO 10-17 09:29:31 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:31 [v1/engine/utils.py:859] HELLO from local core engine process 1.
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp1
[1;36m(EngineCore_DP3 pid=275)[0;0m INFO 10-17 09:29:31 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:31 [v1/engine/utils.py:859] HELLO from local core engine process 3.
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:31 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp3
[1;36m(EngineCore_DP7 pid=279)[0;0m INFO 10-17 09:29:32 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:32 [v1/engine/utils.py:859] HELLO from local core engine process 7.
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp7
[1;36m(EngineCore_DP2 pid=274)[0;0m INFO 10-17 09:29:32 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:32 [v1/engine/utils.py:859] HELLO from local core engine process 2.
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp2
[1;36m(EngineCore_DP0 pid=272)[0;0m INFO 10-17 09:29:32 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/9ccd266e-0791-4d8a-9333-39e0b5caaf1d'], outputs=['ipc:///tmp/74274212-fc88-43d1-9e81-b27000200bff'], coordinator_input='ipc:///tmp/d14245d4-743c-4538-ac1a-5d1cbb821b14', coordinator_output='ipc:///tmp/6c4d0471-ae7e-4eb8-a525-864a6766e533', frontend_stats_publish_address='ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165'), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 44319, '_data_parallel_master_port_list': [58787, 40535, 55863, 57787], 'data_parallel_size': 8})
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:32 [v1/engine/utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:487] Has DP Coordinator: True, stats publish address: ipc:///tmp/a75c4ce0-8565-4af3-b6cc-f8934a398165
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:32 [v1/engine/core.py:985] Setting kv_transfer_config.engine_id to da373fea-13d3-4528-92f5-f0d0b154edcc_dp0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP2 pid=274)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=272)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP2 pid=274)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=272)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/810c17b2-058c-4962-b931-f4b96085f104
[1;36m(EngineCore_DP0 pid=272)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/89ee5209-9dcd-4e00-bc77-d151bb0bbc0f
[1;36m(EngineCore_DP0 pid=272)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_54923cb2'), local_subscribe_addr='ipc:///tmp/89ee5209-9dcd-4e00-bc77-d151bb0bbc0f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP2 pid=274)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_7e4834e0'), local_subscribe_addr='ipc:///tmp/810c17b2-058c-4962-b931-f4b96085f104', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:33 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP7 pid=279)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP4 pid=276)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP6 pid=278)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP7 pid=279)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP6 pid=278)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP4 pid=276)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/b15d016e-681d-4e33-a1d0-cec8051750aa
[1;36m(EngineCore_DP4 pid=276)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/5a0303da-8f15-446b-8063-9e335df95ad3
[1;36m(EngineCore_DP6 pid=278)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/12f4fc21-fea0-47f1-aea6-1bfc9d19cfdb
[1;36m(EngineCore_DP4 pid=276)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_e5ea8578'), local_subscribe_addr='ipc:///tmp/5a0303da-8f15-446b-8063-9e335df95ad3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP6 pid=278)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_edf1f577'), local_subscribe_addr='ipc:///tmp/12f4fc21-fea0-47f1-aea6-1bfc9d19cfdb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP7 pid=279)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_4fc70264'), local_subscribe_addr='ipc:///tmp/b15d016e-681d-4e33-a1d0-cec8051750aa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP3 pid=275)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP3 pid=275)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP5 pid=277)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP5 pid=277)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/47cb6e96-349e-4ea8-b6bf-4f69845f4808
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/007d2bf0-7593-4e31-8603-0285177e08e7
[1;36m(EngineCore_DP3 pid=275)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_46504a03'), local_subscribe_addr='ipc:///tmp/47cb6e96-349e-4ea8-b6bf-4f69845f4808', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP5 pid=277)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_70f189c5'), local_subscribe_addr='ipc:///tmp/007d2bf0-7593-4e31-8603-0285177e08e7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP1 pid=273)[0;0m INFO 10-17 09:29:33 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='deepseek-ai/DeepSeek-V3-0324', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-V3-0324', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-V3-0324, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,0],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP1 pid=273)[0;0m WARNING 10-17 09:29:33 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 160 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP1 pid=273)[0;0m DEBUG 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/95195c08-933d-49b4-bf44-501879639a38
[1;36m(EngineCore_DP1 pid=273)[0;0m INFO 10-17 09:29:33 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_cfc89ca6'), local_subscribe_addr='ipc:///tmp/95195c08-933d-49b4-bf44-501879639a38', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-17 09:29:35 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:35 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:35 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:35 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:35 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:35 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:35 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:35 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:35 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:35 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:35 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:35 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:35 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:36 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:36 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:36 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:36 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:36 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:36 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:36 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:36 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:36 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:36 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:36 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:36 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:36 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:36 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:36 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:36 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:36 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:36 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:36 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:36 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:37 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-17 09:29:37 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-17 09:29:37 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:37 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:37 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:37 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:37 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:37 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:37 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-17 09:29:37 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-17 09:29:37 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-17 09:29:37 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 10-17 09:29:37 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 10-17 09:29:38 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:38 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:38 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:38 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:38 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:38 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:38 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:38 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:39 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:39 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:39 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:39 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
W1017 09:29:39.408000 1496 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:39.408000 1496 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:39 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:39 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:39 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:39 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:39 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
W1017 09:29:40.065000 1488 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:40.065000 1488 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:40 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79396951b6e0>
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/89ee5209-9dcd-4e00-bc77-d151bb0bbc0f
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/7922d714-dbdc-4217-84b4-f8fc4a489c5d
INFO 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_26edda5f'), local_subscribe_addr='ipc:///tmp/7922d714-dbdc-4217-84b4-f8fc4a489c5d', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-17 09:29:40 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ed099d19b50>
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/12f4fc21-fea0-47f1-aea6-1bfc9d19cfdb
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/ed0c1e61-2e27-4527-ba7e-c81296f448b5
INFO 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9d31528f'), local_subscribe_addr='ipc:///tmp/ed0c1e61-2e27-4527-ba7e-c81296f448b5', remote_subscribe_addr=None, remote_addr_ipv6=False)
W1017 09:29:40.726000 1489 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:40.726000 1489 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:40 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b09680a1b50>
DEBUG 10-17 09:29:40 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:40 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/810c17b2-058c-4962-b931-f4b96085f104
DEBUG 10-17 09:29:40 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/893849e5-5142-402a-9ade-a9a8df1c664d
INFO 10-17 09:29:40 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eaecb606'), local_subscribe_addr='ipc:///tmp/893849e5-5142-402a-9ade-a9a8df1c664d', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-17 09:29:40 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:40 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:40 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:40 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:41 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:41 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:41 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:41 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
W1017 09:29:41.599000 1507 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:41.599000 1507 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:41 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:41 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:41 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-17 09:29:41 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-17 09:29:41 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-17 09:29:41 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:41 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x755a75908680>
DEBUG 10-17 09:29:41 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/95195c08-933d-49b4-bf44-501879639a38
DEBUG 10-17 09:29:41 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/f888dd2d-71ad-4cdd-8176-2387dcf3b55a
INFO 10-17 09:29:41 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_64eed7ba'), local_subscribe_addr='ipc:///tmp/f888dd2d-71ad-4cdd-8176-2387dcf3b55a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:42 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
W1017 09:29:42.450000 1504 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:42.450000 1504 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:42 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:42 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:42 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e4e1e019490>
DEBUG 10-17 09:29:42 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/007d2bf0-7593-4e31-8603-0285177e08e7
DEBUG 10-17 09:29:42 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/d1993e9d-5612-4011-ac17-fd2819f6900a
INFO 10-17 09:29:42 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_012feb24'), local_subscribe_addr='ipc:///tmp/d1993e9d-5612-4011-ac17-fd2819f6900a', remote_subscribe_addr=None, remote_addr_ipv6=False)
W1017 09:29:43.104000 1497 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:43.104000 1497 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:43 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:43 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:43 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x725412c764e0>
DEBUG 10-17 09:29:43 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/5a0303da-8f15-446b-8063-9e335df95ad3
DEBUG 10-17 09:29:43 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/92752e52-a156-4e4d-88d8-4782d49dc120
INFO 10-17 09:29:43 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fee90b14'), local_subscribe_addr='ipc:///tmp/92752e52-a156-4e4d-88d8-4782d49dc120', remote_subscribe_addr=None, remote_addr_ipv6=False)
W1017 09:29:43.931000 1503 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:43.931000 1503 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:44 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:44 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:44 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b7a0d2d9b50>
DEBUG 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/47cb6e96-349e-4ea8-b6bf-4f69845f4808
DEBUG 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/6fd662e0-cddf-407b-a62d-072b8ee3ce92
INFO 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b863392f'), local_subscribe_addr='ipc:///tmp/6fd662e0-cddf-407b-a62d-072b8ee3ce92', remote_subscribe_addr=None, remote_addr_ipv6=False)
W1017 09:29:44.668000 1498 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1017 09:29:44.668000 1498 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 10-17 09:29:44 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-17 09:29:44 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-17 09:29:44 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x717b2c8fbb60>
DEBUG 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/b15d016e-681d-4e33-a1d0-cec8051750aa
DEBUG 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/475afa21-471e-408c-a702-1e77e29444f0
INFO 10-17 09:29:44 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c683d0b0'), local_subscribe_addr='ipc:///tmp/475afa21-471e-408c-a702-1e77e29444f0', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:56239 backend=nccl
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50631 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=0 distributed_init_method=tcp://127.0.0.1:55863 for DP
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=6 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:48243 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=2 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33659 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=1 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34791 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=4 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:47533 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=5 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:40983 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=3 distributed_init_method=tcp://127.0.0.1:55863 for DP
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:55795 backend=nccl
INFO 10-17 09:29:45 [distributed/parallel_state.py:1047] Adjusting world_size=8 rank=7 distributed_init_method=tcp://127.0.0.1:55863 for DP
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
DEBUG 10-17 09:29:45 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:29:45 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:29:45 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:29:52 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:02 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:12 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:22 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-17 09:30:22 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:32 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:42 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:30:52 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 7 in world size 8 is assigned as DP rank 7, PP rank 0, TP rank 0, EP rank 7
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 1 in world size 8 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 2 in world size 8 is assigned as DP rank 2, PP rank 0, TP rank 0, EP rank 2
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 6 in world size 8 is assigned as DP rank 6, PP rank 0, TP rank 0, EP rank 6
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 3 in world size 8 is assigned as DP rank 3, PP rank 0, TP rank 0, EP rank 3
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 5 in world size 8 is assigned as DP rank 5, PP rank 0, TP rank 0, EP rank 5
INFO 10-17 09:30:58 [distributed/device_communicators/cuda_communicator.py:112] Using DeepEP Low-Latency all2all manager.
INFO 10-17 09:30:58 [distributed/parallel_state.py:1208] rank 4 in world size 8 is assigned as DP rank 4, PP rank 0, TP rank 0, EP rank 4
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp0
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp3
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp0
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp3
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp6
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp6
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp4
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp1
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp4
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp1
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp2
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp2
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp7
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp7
INFO 10-17 09:30:58 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: da373fea-13d3-4528-92f5-f0d0b154edcc_dp5
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 10-17 09:30:58 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker da373fea-13d3-4528-92f5-f0d0b154edcc_dp5
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:30:58 NIXL INFO    _api.py:354 Backend UCX was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:244 Initialized NIXL agent: 0863c33c-2a34-4cfa-b33b-ab2a06e84e8c
INFO 10-17 09:31:01 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:01 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
2025-10-17 09:31:01 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:244 Initialized NIXL agent: ffbe694b-6976-4238-b57d-30d851d717a8
INFO 10-17 09:31:01 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:01 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
2025-10-17 09:31:01 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:244 Initialized NIXL agent: 6e15e730-7503-4e28-86c7-3fa76c4bc5e9
INFO 10-17 09:31:01 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:01 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
2025-10-17 09:31:01 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:244 Initialized NIXL agent: 6518f4e7-2bdd-4d57-9edf-b3920f6c14b6
INFO 10-17 09:31:01 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:01 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
2025-10-17 09:31:01 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:01 NIXL INFO    _api.py:244 Initialized NIXL agent: 8df55182-0e0f-42c6-b3a7-7b3e78f12462
INFO 10-17 09:31:01 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:01 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:01 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
2025-10-17 09:31:02 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:02 NIXL INFO    _api.py:244 Initialized NIXL agent: bdff6156-fb74-43a4-9642-344bd9bce473
INFO 10-17 09:31:02 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:02 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
2025-10-17 09:31:02 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:02 NIXL INFO    _api.py:244 Initialized NIXL agent: bd158e61-18a3-4420-8193-a63d384456af
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
INFO 10-17 09:31:02 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
2025-10-17 09:31:02 NIXL INFO    _api.py:354 Backend GDS was instantiated
2025-10-17 09:31:02 NIXL INFO    _api.py:244 Initialized NIXL agent: 3caed43a-c8de-484a-b86c-a74ad37bb7d6
INFO 10-17 09:31:02 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
INFO 10-17 09:31:02 [platforms/cuda.py:288] Using FlashMLA backend on V1 engine.
INFO 10-17 09:31:02 [distributed/.../kv_connector/utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASHMLA
DEBUG 10-17 09:31:02 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout NHD
[1;36m(APIServer pid=1)[0;0m DEBUG 10-17 09:31:02 [v1/engine/utils.py:776] Waiting for 8 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
INFO 10-17 09:31:02 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
DEBUG 10-17 09:31:02 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m DEBUG 10-17 09:31:02 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m DEBUG 10-17 09:31:02 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m DEBUG 10-17 09:31:02 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m INFO 10-17 09:31:02 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 6/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->192, 1->193, 2->194, 3->195, 4->196, 5->197, 6->198, 7->199, 8->200, 9->201, 10->202, 11->203, 12->204, 13->205, 14->206, 15->207, 16->208, 17->209, 18->210, 19->211, 20->212, 21->213, 22->214, 23->215, 24->216, 25->217, 26->218, 27->219, 28->220, 29->221, 30->222, 31->223.
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m DEBUG 10-17 09:31:02 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m INFO 10-17 09:31:02 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m WARNING 10-17 09:31:02 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m INFO 10-17 09:31:02 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 0/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15, 16->16, 17->17, 18->18, 19->19, 20->20, 21->21, 22->22, 23->23, 24->24, 25->25, 26->26, 27->27, 28->28, 29->29, 30->30, 31->31.
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m DEBUG 10-17 09:31:02 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m INFO 10-17 09:31:02 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m WARNING 10-17 09:31:02 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m INFO 10-17 09:31:02 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 1/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47, 16->48, 17->49, 18->50, 19->51, 20->52, 21->53, 22->54, 23->55, 24->56, 25->57, 26->58, 27->59, 28->60, 29->61, 30->62, 31->63.
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m DEBUG 10-17 09:31:02 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m INFO 10-17 09:31:02 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m WARNING 10-17 09:31:02 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m DEBUG 10-17 09:31:02 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m DEBUG 10-17 09:31:02 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m INFO 10-17 09:31:02 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 3/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103, 8->104, 9->105, 10->106, 11->107, 12->108, 13->109, 14->110, 15->111, 16->112, 17->113, 18->114, 19->115, 20->116, 21->117, 22->118, 23->119, 24->120, 25->121, 26->122, 27->123, 28->124, 29->125, 30->126, 31->127.
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m DEBUG 10-17 09:31:02 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m INFO 10-17 09:31:02 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m WARNING 10-17 09:31:02 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m INFO 10-17 09:31:02 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m INFO 10-17 09:31:03 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 4/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->128, 1->129, 2->130, 3->131, 4->132, 5->133, 6->134, 7->135, 8->136, 9->137, 10->138, 11->139, 12->140, 13->141, 14->142, 15->143, 16->144, 17->145, 18->146, 19->147, 20->148, 21->149, 22->150, 23->151, 24->152, 25->153, 26->154, 27->155, 28->156, 29->157, 30->158, 31->159.
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m DEBUG 10-17 09:31:03 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m INFO 10-17 09:31:03 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m WARNING 10-17 09:31:03 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m INFO 10-17 09:31:03 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m INFO 10-17 09:31:03 [v1/worker/gpu_model_runner.py:2602] Starting to load model deepseek-ai/DeepSeek-V3-0324...
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m INFO 10-17 09:31:03 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m INFO 10-17 09:31:03 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m DEBUG 10-17 09:31:03 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m INFO 10-17 09:31:03 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 7/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->224, 1->225, 2->226, 3->227, 4->228, 5->229, 6->230, 7->231, 8->232, 9->233, 10->234, 11->235, 12->236, 13->237, 14->238, 15->239, 16->240, 17->241, 18->242, 19->243, 20->244, 21->245, 22->246, 23->247, 24->248, 25->249, 26->250, 27->251, 28->252, 29->253, 30->254, 31->255.
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m DEBUG 10-17 09:31:03 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m INFO 10-17 09:31:03 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m WARNING 10-17 09:31:03 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m DEBUG 10-17 09:31:03 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m INFO 10-17 09:31:03 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m INFO 10-17 09:31:03 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 2/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71, 8->72, 9->73, 10->74, 11->75, 12->76, 13->77, 14->78, 15->79, 16->80, 17->81, 18->82, 19->83, 20->84, 21->85, 22->86, 23->87, 24->88, 25->89, 26->90, 27->91, 28->92, 29->93, 30->94, 31->95.
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m DEBUG 10-17 09:31:03 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m INFO 10-17 09:31:03 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m WARNING 10-17 09:31:03 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP0_EP0 pid=1488)[0;0m INFO 10-17 09:31:03 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP6_EP6 pid=1496)[0;0m INFO 10-17 09:31:03 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP1_EP1 pid=1507)[0;0m INFO 10-17 09:31:03 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m DEBUG 10-17 09:31:03 [v1/attention/.../mla/common.py:1128] Using FlashAttention prefill for MLA
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m INFO 10-17 09:31:03 [model_executor/.../fused_moe/layer.py:1052] [EP Rank 5/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/256. Experts local to global index map: 0->160, 1->161, 2->162, 3->163, 4->164, 5->165, 6->166, 7->167, 8->168, 9->169, 10->170, 11->171, 12->172, 13->173, 14->174, 15->175, 16->176, 17->177, 18->178, 19->179, 20->180, 21->181, 22->182, 23->183, 24->184, 25->185, 26->186, 27->187, 28->188, 29->189, 30->190, 31->191.
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m DEBUG 10-17 09:31:03 [model_executor/.../fused_moe/config.py:752] Using FusedMoEConfig::max_num_tokens=512
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m INFO 10-17 09:31:03 [model_executor/.../quantization/fp8.py:462] Using DeepGemm kernels for Fp8MoEMethod.
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m WARNING 10-17 09:31:03 [model_executor/.../quantization/fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP3_EP3 pid=1503)[0;0m INFO 10-17 09:31:03 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m ERROR 10-17 09:31:03 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP4_EP4 pid=1497)[0;0m INFO 10-17 09:31:03 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[rank0]:[W1017 09:31:04.435703756 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W1017 09:31:04.569285376 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W1017 09:31:04.570164554 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W1017 09:31:04.796196681 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP7_EP7 pid=1498)[0;0m INFO 10-17 09:31:04 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]:[W1017 09:31:04.862391290 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(Worker_DP5_EP5 pid=1504)[0;0m INFO 10-17 09:31:04 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] WorkerProc failed to start.
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] Traceback (most recent call last):
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 571, in worker_main
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     worker = WorkerProc(*args, **kwargs)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 437, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.worker.load_model()
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2635, in load_model
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = model_loader.load_model(
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 45, in load_model
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     model = initialize_model(vllm_config=vllm_config,
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1208, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.model = DeepseekV2Model(vllm_config=vllm_config,
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 201, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1135, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                                     ^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 630, in make_layers
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1137, in <lambda>
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     lambda prefix: DeepseekV2DecoderLayer(vllm_config, prefix,
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 1037, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.mlp = DeepseekV2MoE(
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                ^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py", line 220, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.experts = SharedFusedMoE(
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                    ^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py", line 25, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     super().__init__(**kwargs)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1140, in __init__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 531, in create_weights
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     w2_weight = torch.nn.Parameter(torch.empty(
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]                                    ^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]     return func(*args, **kwargs)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m ERROR 10-17 09:31:04 [v1/executor/multiproc_executor.py:597] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 89.06 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.91 GiB is allocated by PyTorch, and 199.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(Worker_DP2_EP2 pid=1489)[0;0m INFO 10-17 09:31:04 [v1/executor/multiproc_executor.py:558] Parent process exited, terminating worker
[rank5]:[W1017 09:31:05.421326317 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W1017 09:31:05.504061100 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W1017 09:31:05.055580788 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=272)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     self._init_executor()
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708]     raise e from None
[1;36m(EngineCore_DP0 pid=272)[0;0m ERROR 10-17 09:31:06 [v1/engine/core.py:708] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP0 pid=272)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=272)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=272)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_DP0 pid=272)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP0 pid=272)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=272)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=272)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=272)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP0 pid=272)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP0 pid=272)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=272)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP0 pid=272)[0;0m     raise e from None
[1;36m(EngineCore_DP0 pid=272)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP6 pid=278)[0;0m Process EngineCore_DP6:
[1;36m(EngineCore_DP6 pid=278)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] EngineCore failed to start.
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_DP6 pid=278)[0;0m     self.run()
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP6 pid=278)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_DP6 pid=278)[0;0m     raise e
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP6 pid=278)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self._init_executor()
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     raise e from None
[1;36m(EngineCore_DP6 pid=278)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP6 pid=278)[0;0m     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP6 pid=278)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP6 pid=278)[0;0m     self._init_executor()
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP6 pid=278)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP6 pid=278)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP6 pid=278)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP6 pid=278)[0;0m     raise e from None
[1;36m(EngineCore_DP6 pid=278)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP1 pid=273)[0;0m Process EngineCore_DP1:
[1;36m(EngineCore_DP1 pid=273)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] EngineCore failed to start.
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP1 pid=273)[0;0m     self.run()
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP1 pid=273)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_DP1 pid=273)[0;0m     raise e
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self._init_executor()
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     raise e from None
[1;36m(EngineCore_DP1 pid=273)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP1 pid=273)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP1 pid=273)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP1 pid=273)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP1 pid=273)[0;0m     self._init_executor()
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP1 pid=273)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP1 pid=273)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP1 pid=273)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP1 pid=273)[0;0m     raise e from None
[1;36m(EngineCore_DP1 pid=273)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] EngineCore failed to start.
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m Process EngineCore_DP4:
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self._init_executor()
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708]     raise e from None
[1;36m(EngineCore_DP4 pid=276)[0;0m ERROR 10-17 09:31:07 [v1/engine/core.py:708] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP4 pid=276)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP4 pid=276)[0;0m     self.run()
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP4 pid=276)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_DP4 pid=276)[0;0m     raise e
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 695, in run_engine_core
[1;36m(EngineCore_DP4 pid=276)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP4 pid=276)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 965, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m     super().__init__(vllm_config, local_client, handshake_address,
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 83, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP4 pid=276)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP4 pid=276)[0;0m     self._init_executor()
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 106, in _init_executor
[1;36m(EngineCore_DP4 pid=276)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP4 pid=276)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 509, in wait_for_ready
[1;36m(EngineCore_DP4 pid=276)[0;0m     raise e from None
[1;36m(EngineCore_DP4 pid=276)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_DP4 pid=276)[0;0m Exception ignored in atexit callback: <function dump_compile_times at 0x78c712e32d40>
[1;36m(EngineCore_DP4 pid=276)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 811, in dump_compile_times
[1;36m(EngineCore_DP4 pid=276)[0;0m     log.info(compile_times(repr="str", aggregate=True))
[1;36m(EngineCore_DP4 pid=276)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP5 pid=277)[0;0m DEBUG 10-17 09:31:07 [v1/engine/core.py:704] EngineCore exiting.
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 797, in compile_times
[1;36m(EngineCore_DP3 pid=275)[0;0m DEBUG 10-17 09:31:07 [v1/engine/core.py:704] EngineCore exiting.
[1;36m(EngineCore_DP4 pid=276)[0;0m     out += tabulate(rows, headers=("Function", "Runtimes (s)"))
[1;36m(EngineCore_DP2 pid=274)[0;0m DEBUG 10-17 09:31:07 [v1/engine/core.py:704] EngineCore exiting.
[1;36m(EngineCore_DP4 pid=276)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 226, in tabulate
[1;36m(EngineCore_DP4 pid=276)[0;0m     import tabulate
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/tabulate/__init__.py", line 543, in <module>
[1;36m(EngineCore_DP4 pid=276)[0;0m     lineabove=Line("", "=", "  ", ""),
[1;36m(EngineCore_DP4 pid=276)[0;0m               ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "<string>", line 1, in <lambda>
[1;36m(EngineCore_DP4 pid=276)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 679, in signal_handler
[1;36m(EngineCore_DP4 pid=276)[0;0m     raise SystemExit()
[1;36m(EngineCore_DP4 pid=276)[0;0m SystemExit: 
[1;36m(EngineCore_DP7 pid=279)[0;0m DEBUG 10-17 09:31:07 [v1/engine/core.py:704] EngineCore exiting.
[1;36m(APIServer pid=1)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/bin/vllm", line 10, in <module>
[1;36m(APIServer pid=1)[0;0m     sys.exit(main())
[1;36m(APIServer pid=1)[0;0m              ^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 54, in main
[1;36m(APIServer pid=1)[0;0m     args.dispatch_function(args)
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 57, in cmd
[1;36m(APIServer pid=1)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=1)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
[1;36m(APIServer pid=1)[0;0m     return runner.run(main)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=1)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=1)[0;0m     return await main
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1884, in run_server
[1;36m(APIServer pid=1)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1902, in run_server_worker
[1;36m(APIServer pid=1)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=1)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 180, in build_async_engine_client
[1;36m(APIServer pid=1)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=1)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 225, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=1)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=1)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1572, in inner
[1;36m(APIServer pid=1)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 207, in from_vllm_config
[1;36m(APIServer pid=1)[0;0m     return cls(
[1;36m(APIServer pid=1)[0;0m            ^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
[1;36m(APIServer pid=1)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=1)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 101, in make_async_mp_client
[1;36m(APIServer pid=1)[0;0m     return DPLBAsyncMPClient(*client_args)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 1125, in __init__
[1;36m(APIServer pid=1)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 975, in __init__
[1;36m(APIServer pid=1)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 769, in __init__
[1;36m(APIServer pid=1)[0;0m     super().__init__(
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 448, in __init__
[1;36m(APIServer pid=1)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=1)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=1)[0;0m     next(self.gen)
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 732, in launch_core_engines
[1;36m(APIServer pid=1)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 785, in wait_for_engine_startup
[1;36m(APIServer pid=1)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=1)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 8 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
