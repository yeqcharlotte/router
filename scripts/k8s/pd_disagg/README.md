# vLLM Router - Prefill/Decode Disaggregation Deployment

Kubernetes deployment manifests and scripts for deploying vLLM Router with Prefill-Decode disaggregation support.

## Files

- `router-pod.yaml` - Single pod deployment (for testing)
- `router-deployment.yaml` - Deployment with replicas (for production)
- `router-service.yaml` - Service definitions (ClusterIP and NodePort)
- `deploy.sh` - Deployment script
- `cleanup.sh` - Cleanup script
- `run-benchmark.sh` - Script to run benchmarks from prefill pod

## Prerequisites

1. Docker image built and pushed to ECR:
   ```bash
   cd ~/router
   docker build -f Dockerfile.router -t vllm-router:dp-fallback .
   docker tag vllm-router:dp-fallback 584868043064.dkr.ecr.us-west-2.amazonaws.com/dev-vllm-repo:router-dp-fallback
   docker push 584868043064.dkr.ecr.us-west-2.amazonaws.com/dev-vllm-repo:router-dp-fallback
   ```

2. Prefill and Decode servers running in the cluster
3. kubectl access to the cluster

## Quick Start

### Deploy as Pod (for testing)

```bash
./deploy.sh pod
```

### Deploy as Deployment (for production)

```bash
./deploy.sh deployment
```

## Configuration

Edit the YAML files to customize:

- `--prefill` and `--decode` URLs - Update with your server IPs/hostnames
- `--intra-node-data-parallel-size` - Set the fallback DP size
- `nodeName` - Set to deploy on specific node (in router-pod.yaml)
- `nodeSelector` - Uncomment in router-deployment.yaml to pin to specific node
- Resources (CPU/memory limits)

## Accessing the Router

### From within cluster

```bash
# Using ClusterIP service
curl http://vllm-router-pd.llm-d-pd.svc.cluster.local:10001/health

# Using NodePort (from any node)
curl http://NODE_IP:30001/health
```

### From outside cluster

```bash
# Port forward
kubectl port-forward -n llm-d-pd pod/vllm-router-pd 10001:10001

# Then access locally
curl http://localhost:10001/health
```

## Monitoring

### View logs

```bash
# Pod deployment
kubectl logs -n llm-d-pd vllm-router-pd -f

# Deployment
kubectl logs -n llm-d-pd -l app=vllm-router -f
```

### Check status

```bash
kubectl get pods -n llm-d-pd -l app=vllm-router
kubectl describe pod vllm-router-pd -n llm-d-pd
```

### Metrics

Prometheus metrics available at:
- ClusterIP: `http://vllm-router-pd.llm-d-pd.svc.cluster.local:29000/metrics`
- NodePort: `http://NODE_IP:30002/metrics`

## Running Benchmarks

Use the included benchmark script to run load tests:

```bash
./run-benchmark.sh
```

This will:
1. Connect to the prefill pod
2. Run vllm bench serve against the router
3. Display results

### Benchmark Results

**Test Configuration:**
- Date: 2025-10-28
- Model: deepseek-ai/DeepSeek-V3
- Input Length: 2000 tokens
- Output Length: 2000 tokens
- Number of Prompts: 100
- Max Concurrency: 16
- Prefill Server: http://10.2.1.155:8000
- Decode Server: http://10.2.1.168:8000
- Router Configuration: `--intra-node-data-parallel-size 1`

**Performance Metrics:**
```
Successful requests:                     100
Maximum request concurrency:             16
Benchmark duration (s):                  331.81
Total input tokens:                      199668
Total generated tokens:                  200000
Request throughput (req/s):              0.30
Output token throughput (tok/s):         602.75
Peak output token throughput (tok/s):    704.00
Peak concurrent requests:                27.00
Total Token throughput (tok/s):          1204.50

Time to First Token (TTFT):
  Mean TTFT (ms):                        689.94
  Median TTFT (ms):                      652.62
  P99 TTFT (ms):                         1681.16

Time per Output Token (excl. 1st token):
  Mean TPOT (ms):                        24.16
  Median TPOT (ms):                      24.49
  P99 TPOT (ms):                         25.06

Inter-token Latency:
  Mean ITL (ms):                         24.16
  Median ITL (ms):                       23.99
  P99 ITL (ms):                          27.03
```

**Key Observations:**
- ‚úÖ All 100 requests completed successfully
- ‚úÖ DP-aware fallback mechanism working correctly (used fallback dp_size=1 when API returned 404)
- ‚úÖ Router deployed on same Kubernetes node as prefill server for optimal network performance
- ‚úÖ Stable token generation throughput (~603 tok/s average, ~704 tok/s peak)
- ‚úÖ Low and consistent inter-token latency (~24ms mean)

### Performance Comparison: vLLM Router vs. Native Gateway

Comparison with llm-d native Istio gateway (same test configuration):

| Metric | vLLM Router | llm-d Gateway | Difference |
|--------|-------------|---------------|------------|
| Successful requests | 100/100 | 100/100 | - |
| Benchmark duration (s) | 331.81 | 326.47 | +1.6% |
| Output token throughput (tok/s) | 602.75 | 612.61 | -1.6% |
| Peak output throughput (tok/s) | 704.00 | 688.00 | +2.3% |
| Mean TTFT (ms) | 689.94 | 248.12 | +178% |
| Median TTFT (ms) | 652.62 | 193.10 | +238% |
| P99 TTFT (ms) | 1681.16 | 438.54 | +283% |
| Mean TPOT (ms) | 24.16 | 23.97 | +0.8% |
| Median TPOT (ms) | 24.49 | 24.10 | +1.6% |
| P99 TPOT (ms) | 25.06 | 24.90 | +0.6% |
| Mean ITL (ms) | 24.16 | 23.97 | +0.8% |
| Median ITL (ms) | 23.99 | 24.15 | -0.7% |
| P99 ITL (ms) | 27.03 | 27.24 | -0.8% |

**Analysis:**
- üéØ **Token generation performance**: Nearly identical (~99% of native gateway throughput)
- üéØ **Time per output token**: Virtually the same (~24ms for both)
- üéØ **Inter-token latency**: Consistent and comparable between both systems
- ‚ö†Ô∏è **Time to first token**: Higher in vLLM Router (+440ms) - likely due to additional routing hop
- ‚úÖ **Overall**: vLLM Router provides production-grade performance with <2% throughput difference

The vLLM Router successfully demonstrates that custom routing logic can be added with minimal performance impact. The higher TTFT is expected due to the additional network hop through the router, but the steady-state token generation performance is nearly identical.

## Cleanup

```bash
# Remove pod only
./cleanup.sh pod

# Remove deployment and services
./cleanup.sh deployment

# Remove everything
./cleanup.sh all
```

## Troubleshooting

### Pod not starting

Check logs and events:
```bash
kubectl logs -n llm-d-pd vllm-router-pd
kubectl describe pod vllm-router-pd -n llm-d-pd
```

### Network connectivity issues

Verify prefill/decode servers are accessible from router pod:
```bash
kubectl exec -n llm-d-pd vllm-router-pd -- curl http://10.2.1.155:8000/health
kubectl exec -n llm-d-pd vllm-router-pd -- curl http://10.2.1.168:8000/health
```

### DP-aware fallback

Check router logs for warnings about DP size fallback:
```bash
kubectl logs -n llm-d-pd vllm-router-pd | grep "dp_size"
```
